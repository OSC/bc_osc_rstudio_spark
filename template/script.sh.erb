#!/bin/bash -l

<%- avail_cores = context.node_type.include?("hugemem") ? 48   : 28  -%>
<%- avail_mem   = context.node_type.include?("hugemem") ? 1528 : 120 -%>
<%- num_workers = context.num_workers.to_i -%>

<%-
  spark_config = {
    "spark.ui.reverseProxy" => "true",
    #"spark.ui.reverseProxyUrl" => "https://ondemand.osc.edu/node/${SPARK_MASTER_HOST}/${SPARK_MASTER_WEBUI_PORT}",
    "spark.authenticate" => "true",
    "spark.authenticate.secret" => "${SPARK_SECRET}",
    # Comment out below when reverse proxy and authentication are added
    # This still starts the Web UI on the master/worker procs, but disables it for
    # the Application driver
    "spark.ui.enabled" => "false",
    # So we need to disable the ability to kill applications
    "spark.ui.killEnabled" => "false",
  }
-%>

#
# Start RStudio Server + Spark cluster
#

# Load the required environment
setup_env () {
  module purge
  module load intel/16.0.3 R/3.3.2 rstudio/1.0.136_server spark/2.1.0
}
setup_env

#
# Launch Spark cluster in standalone mode
#

# Create log directory
export LOG_ROOT="${PWD}/logs"
mkdir "${LOG_ROOT}"

# Set master connection information
export SPARK_MASTER_HOST=${host}
export SPARK_MASTER_PORT=$(find_port ${SPARK_MASTER_HOST})
export SPARK_MASTER_WEBUI_PORT=$(find_port ${SPARK_MASTER_HOST})
export MASTER_LOG_FILE="${LOG_ROOT}/spark-master-${SPARK_MASTER_HOST}.out"

# Generate Spark secret
SPARK_SECRET="$(create_passwd)"

# Generate Spark configuration file
export SPARK_CONFIG_FILE="${PWD}/spark-defaults.conf"
(
umask 077
cat > "${SPARK_CONFIG_FILE}" << EOL
<%- spark_config.each do |k, v| -%>
<%= k %> <%= v %>
<%- end -%>
EOL
)

# Launch master
echo "Launching master on ${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}..."
set -x
"${SPARK_HOME}/bin"/spark-class "org.apache.spark.deploy.master.Master" \
    --properties-file "${SPARK_CONFIG_FILE}" \
  &> "${MASTER_LOG_FILE}" &
{ set +x; } 2>/dev/null

# Wait for the master server to fully start
echo "Waiting for master server to open port ${SPARK_MASTER_PORT}..."
if wait_until_port_used "${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}"; then
  echo "Discovered master listening on port ${SPARK_MASTER_PORT}!"
else
  echo "Timed out waiting for master to open port ${SPARK_MASTER_PORT}!" ; exit 1
fi
sleep 2

# Launch workers
echo "Launching workers..."
for ((i=0; i<<%= num_workers %>; i++)); do
  pbsdsh -u bash -l -c "
    <%- if context.only_driver_on_root == "1" -%>
    [[ \${PBS_NODENUM} == "0" ]] && exit 0
    <%- end -%>
    $(declare -f source_helpers)
    source_helpers

    # Load the required environment
    $(declare -f setup_env)
    setup_env

    # Set worker connection information
    export SPARK_WORKER_HOST=\${HOSTNAME}
    export SPARK_WORKER_PORT=\$(find_port \${SPARK_WORKER_HOST})
    export SPARK_WORKER_WEBUI_PORT=\$(find_port \${SPARK_WORKER_HOST})
    export SPARK_WORKER_DIR=\"${PWD}/work\"
    export SPARK_WORKER_CORES=<%= avail_cores / num_workers %>
    export WORKER_LOG_FILE=\"${LOG_ROOT}/spark-worker-\${SPARK_WORKER_HOST}-${i}.out\"

    # Launch worker
    echo \"Launching worker #${i} on \${SPARK_WORKER_HOST}:\${SPARK_WORKER_PORT}...\"
    set -x
    \"${SPARK_HOME}/bin\"/spark-class \"org.apache.spark.deploy.worker.Worker\" \
        --properties-file \"${SPARK_CONFIG_FILE}\" \
        spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} \
      &> \"\${WORKER_LOG_FILE}\"
  " &
done

#
# Launch RStudio Server
#

# Generate an R profile that sets up Spark
export R_PROFILE_USER="${PWD}/shell.R"
(
umask 077
sed 's/^ \{2\}//' > "${R_PROFILE_USER}" << EOL
  .First <- function() {
    # create local user library path (not present by default)
    dir.create(path = Sys.getenv("R_LIBS_USER"), showWarnings = FALSE, recursive = TRUE)

    # add it to the libPaths
    .libPaths(c(Sys.getenv("R_LIBS_USER"), .libPaths()))

    library(sparklyr)

    conf <- spark_config()
    <%- spark_config.each do |k, v| -%>
    conf\$<%= k %> <- "<%= v %>"
    <%- end -%>
    conf\$spark.driver.memory <- "<%= context.only_driver_on_root == "1" ? avail_mem : 2 %>G"
    conf\$spark.executor.memory <- "<%= avail_mem / num_workers %>G"
    conf\$spark.driver.maxResultSize <- "0"

    sc <- spark_connect(master="spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}",
                  config = conf)
    assign("sc", sc, envir = .GlobalEnv)
    cat("\\n Welcome to")
    cat("\\n")
    cat("    ____              __", "\\n")
    cat("   / __/__  ___ _____/ /__", "\\n")
    cat("  _\\\\ \\\\/ _ \\\\/ _ \`/ __/  '_/", "\\n")
    cat(" /___/ .__/\\\\_,_/_/ /_/\\\\_\\\\")
    cat("   version ", invoke(spark_context(sc), "version"), "\\n")
    cat("    /_/", "\\n")
    cat("\\n")
    cat("\\n Spark Session available as 'sc'.\\n")
  }
EOL
)

# Generate an `rsession` wrapper script
export RSESSION_WRAPPER_FILE="${PWD}/rsession.sh"
(
umask 077
sed 's/^ \{2\}//' > "${RSESSION_WRAPPER_FILE}" << EOL
  #!/bin/bash -l

  # Log all output from this script
  export RSESSION_LOG_FILE="${PWD}/rsession.log"
  exec &>"\${RSESSION_LOG_FILE}"

  # Load the required environment
  $(declare -f setup_env)
  setup_env

  # Load the PBS environment
  $(declare -xp | grep PBS)

  # Carry over R_PROFILE_USER
  $(declare -xp R_PROFILE_USER)
  export R_HISTFILE=/dev/null

  # Some debugging info
  module list

  # Launch the original command
  echo "Launching rsession..."
  set -x
  exec rsession "\${@}"
EOL
)
chmod 700 "${RSESSION_WRAPPER_FILE}"

# Set working directory to home directory
cd "${HOME}"

# Launch the RStudio Server
echo "Starting up rserver..."
set -x
/usr/local/proot/bin/proot-x86_64 -b "${TMPDIR}:/tmp" \
  rserver \
    --www-port ${port} \
    --auth-none 0 \
    --auth-pam-helper-path "<%= session.staged_root.join("bin", "auth") %>" \
    --auth-encrypt-password 0 \
    --rsession-path "${RSESSION_WRAPPER_FILE}"
